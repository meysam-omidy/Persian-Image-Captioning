{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data Ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def preprocess(text):\n",
    "    t = re.sub(r'\\(.*?\\)', ' ', str(text))\n",
    "    t = re.sub('[،\\t\\u200c\\u200e\\xa0]',' ', t)\n",
    "    t = re.sub('[#;&،»«؛/\\\\\\']', ' ', t)\n",
    "    t = re.sub('[!؟:].', ' ', t)\n",
    "    t = re.sub('[0-9]', '_', t)\n",
    "    t = re.sub('[a-z]', '_', t)\n",
    "    t = re.sub(r'_+', 'یک', t)\n",
    "    t = re.sub(r'-', ' ', t)\n",
    "    t = re.sub(r' +', ' ', t)\n",
    "    return t\n",
    "\n",
    "data = ImageFolder('persian_image_captioning/',transform=transforms.Compose([\n",
    "        transforms.Resize((140, 200)), transforms.ToTensor()\n",
    "]))\n",
    "image_names = {x.split('\\\\')[1]:i for i,x in enumerate(np.array(data.imgs)[:,0])}\n",
    "with open('persian_image_captioning/news.json', encoding='utf8') as f:\n",
    "    json_file = json.load(f)\n",
    "title_images = []\n",
    "words = set()\n",
    "for x in json_file:\n",
    "    title_images.append([x['title'], x['images']])\n",
    "    words.update(preprocess(x['title']).split())\n",
    "words.update(['<start>','<end>'])\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "for i,w in enumerate(words):\n",
    "    word_to_idx[w] = i\n",
    "    idx_to_word[i] = w\n",
    "titles = []\n",
    "image_indexes = []\n",
    "for title,images in title_images:\n",
    "    t = preprocess(title)\n",
    "    if t in titles:\n",
    "        continue\n",
    "    for image in images:\n",
    "        if image == '1400070516181726323700473.jpg':\n",
    "            continue\n",
    "        image_index = image_names[image]\n",
    "        if image_index in image_indexes:\n",
    "            continue\n",
    "        titles.append(t)\n",
    "        image_indexes.append(image_index)\n",
    "title_tokens = [[] for i in range(17)]\n",
    "image_indexes_splitted = [[] for i in range(17)]\n",
    "for title,image_index in zip(titles, image_indexes):\n",
    "    s = title.split()\n",
    "    arr = [word_to_idx['<start>']]\n",
    "    arr.extend([word_to_idx[x] for x in s])\n",
    "    arr.extend([word_to_idx['<end>']])\n",
    "    title_tokens[len(s)-1].append(arr)\n",
    "    image_indexes_splitted[len(s)-1].append(image_index)\n",
    "TRAIN_TOKENS_DATALOADER = []\n",
    "TRAIN_IMAGES_DATALOADER = []\n",
    "TEST_TOKENS_DATALOADER = []\n",
    "TEST_IMAGES_DATALOADER = []\n",
    "for t,i in zip(title_tokens, image_indexes_splitted):\n",
    "    train_indexes = [x for x in range(len(t))]\n",
    "    test_indexes = random.sample(train_indexes, int(0.2*len(train_indexes)))\n",
    "    for x in test_indexes:\n",
    "        train_indexes.remove(x)\n",
    "    train_t = np.array(t)[train_indexes]\n",
    "    train_i = np.array(i)[train_indexes]\n",
    "    test_t = np.array(t)[test_indexes]\n",
    "    test_i = np.array(i)[test_indexes]\n",
    "    TRAIN_TOKENS_DATALOADER.append(DataLoader(np.array(train_t).reshape(len(train_t), -1), batch_size=64, shuffle=False))\n",
    "    TRAIN_IMAGES_DATALOADER.append(DataLoader(Subset(data, train_i), batch_size=64, shuffle=False))\n",
    "    TEST_TOKENS_DATALOADER.append(DataLoader(np.array(test_t).reshape(len(test_t), -1), batch_size=64, shuffle=False))\n",
    "    TEST_IMAGES_DATALOADER.append(DataLoader(Subset(data, test_i), batch_size=64, shuffle=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.optim import Adam\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, feature_dim, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim + feature_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, tokens, hidden, c):\n",
    "        embeddings = self.embedding(tokens)\n",
    "        f = torch.zeros(embeddings.size(1), features.size(0), features.size(1)).to(DEVICE)\n",
    "        f[:] = features\n",
    "        f = f.permute(1,0,2)\n",
    "        lstm_input = torch.cat((embeddings, f), dim=2)\n",
    "        lstm_output, (hidden_, c_) = self.lstm(lstm_input, (hidden, c))\n",
    "        outputs = self.fc(lstm_output)\n",
    "        outputs = outputs.permute(0,2,1)\n",
    "        return outputs, hidden_, c_\n",
    "\n",
    "class Img2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def choose_guess_target(self, guess, target, ratio):\n",
    "        arr = []\n",
    "        for i in range(guess.shape[0]):\n",
    "            if random.random() < ratio:\n",
    "                arr.append(guess[i])\n",
    "            else:\n",
    "                arr.append(target[i])\n",
    "        return torch.tensor(arr).unsqueeze(1).to(DEVICE)\n",
    "\n",
    "    def forward(self, images, tokens, ratio=0.7):\n",
    "        features = self.encoder(images)\n",
    "        outputs = torch.zeros(tokens.shape[1], tokens.shape[0]).to(DEVICE)\n",
    "        logits = torch.zeros(tokens.shape[1], tokens.shape[0], self.vocab_size).to(DEVICE)\n",
    "        best_guess = tokens[:,0]\n",
    "        hidden = c = torch.zeros((1, tokens.shape[0], self.decoder.lstm.hidden_size)).to(DEVICE)\n",
    "        for t in range(tokens.shape[1]):\n",
    "            x = self.choose_guess_target(best_guess.unsqueeze(1), tokens[:,t].unsqueeze(1), ratio).type(torch.int)\n",
    "            predictions, hidden, c = self.decoder(features, x, hidden, c)\n",
    "            logits[t] = predictions.squeeze(2)\n",
    "            best_guess = predictions.squeeze(2).argmax(dim=-1)\n",
    "            outputs[t] = best_guess\n",
    "        return outputs.permute(1,0), logits.permute(1,2,0)\n",
    "    \n",
    "    def generate(self, image, start_token, end_token, max_length):\n",
    "        features = self.encoder(image.unsqueeze(0))\n",
    "        x = torch.tensor([[start_token]], dtype=torch.int).to(DEVICE)\n",
    "        outputs = []\n",
    "        hidden = c = torch.zeros((1, 1, self.decoder.lstm.hidden_size)).to(DEVICE)\n",
    "        for t in range(max_length):\n",
    "            predictions, hidden, c = self.decoder(features, x, hidden, c)\n",
    "            predictions = predictions[:, :, -1].unsqueeze(2)\n",
    "            if predictions.argmax(dim=1).item() == end_token:\n",
    "                break\n",
    "            else:\n",
    "                outputs.append(predictions.argmax(dim=1).item())\n",
    "            x = torch.concatenate([x, predictions.argmax(dim=1)], dim=-1)\n",
    "        return outputs\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "encoder = models.resnet18()\n",
    "encoder.fc = nn.Linear(encoder.fc.in_features, 256)\n",
    "decoder = Decoder(len(words), 50, 256, 256).to(DEVICE)\n",
    "img2seq = Img2Seq(len(words), encoder, decoder).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "encoder_optimizer = Adam(encoder.fc.parameters(), lr=0.0008)\n",
    "decoder_optimizer = Adam(decoder.parameters(), lr=0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    b1 = 0\n",
    "    train_acc_sum = 0\n",
    "    for image_loader, token_loader in zip(TRAIN_IMAGES_DATALOADER, TRAIN_TOKENS_DATALOADER):\n",
    "        for i,((images,_), tokens) in enumerate(zip(image_loader, token_loader)):\n",
    "            tokens = tokens.type(torch.long).to(DEVICE)\n",
    "            expected = tokens[:, 1:]\n",
    "            images = images.to(DEVICE)\n",
    "            output, pred = img2seq(images, tokens[:, :-1], ratio=(0.2+(0.8-0.2)/n_epochs*epoch))\n",
    "            loss = criterion(pred, expected)\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            train_acc_sum += ((torch.softmax(pred, dim=1).argmax(dim=1) == expected).sum() / expected.size(1)).item()\n",
    "            b1 += len(tokens)\n",
    "\n",
    "    b2 = 0\n",
    "    test_acc_sum = 0\n",
    "    for image_loader, token_loader in zip(TEST_IMAGES_DATALOADER, TEST_TOKENS_DATALOADER):\n",
    "        for i,((images,_), tokens) in enumerate(zip(image_loader, token_loader)):\n",
    "            tokens = tokens.type(torch.long).to(DEVICE)\n",
    "            expected = tokens[:, 1:]\n",
    "            images = images.to(DEVICE)\n",
    "            output, pred = img2seq(images, tokens[:, :-1], ratio=0)\n",
    "            test_acc_sum += ((torch.softmax(pred, dim=1).argmax(dim=1) == expected).sum() / expected.size(1)).item()\n",
    "            b2 += len(tokens)\n",
    "    print(f'epoch {epoch}   train accuracy {train_acc_sum/b1}   test accuracy {test_acc_sum/b2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    img2seq.eval()\n",
    "    img = Image.open(image_path)\n",
    "    img = transform(img).to(DEVICE)\n",
    "    generated_sequence = img2seq.generate(img, word_to_idx['<start>'], word_to_idx['<end>'], 17)\n",
    "    caption = ''\n",
    "    for idx in generated_sequence:\n",
    "        caption += f'{idx_to_word[idx]} '\n",
    "    return caption\n",
    "\n",
    "image_path = './persian_image_captioning/selected_images/1.jpg'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((280, 400)), transforms.ToTensor()\n",
    "])\n",
    "generate_caption(image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
